{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6633a8d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polygon-api-client\n",
      "  Using cached polygon_api_client-1.10.1-py3-none-any.whl (39 kB)\n",
      "Collecting websockets<12.0,>=10.3\n",
      "  Using cached websockets-11.0.3-cp39-cp39-macosx_11_0_arm64.whl (121 kB)\n",
      "Collecting certifi<2023.0.0,>=2022.5.18\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting urllib3<2.0.0,>=1.26.9\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: websockets, urllib3, certifi, polygon-api-client\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 11.0.3\n",
      "    Uninstalling websockets-11.0.3:\n",
      "      Successfully uninstalled websockets-11.0.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: polygon-api-client\n",
      "    Found existing installation: polygon-api-client 1.10.1\n",
      "    Uninstalling polygon-api-client-1.10.1:\n",
      "      Successfully uninstalled polygon-api-client-1.10.1\n",
      "Successfully installed certifi-2022.12.7 polygon-api-client-1.10.1 urllib3-1.26.16 websockets-11.0.3\n",
      "Requirement already satisfied: requests in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: matplotlib in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (3.6.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (1.23.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/esajor/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Import Tensorflow 2.0\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "#Install Stock API Client (forex aggregates not currently implemented however I addressed the issue and waiting for them to get back to me. Using requests for now)\n",
    "!pip install --upgrade --force-reinstall polygon-api-client\n",
    "!pip install requests\n",
    "!pip install matplotlib\n",
    "\n",
    "#Import other packages\n",
    "from polygon import RESTClient\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import functools\n",
    "import csv\n",
    "import glob\n",
    "#from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08181f61",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260fa9b",
   "metadata": {},
   "source": [
    "## **1.1 TWO FUNCTIONS: ONE TO ACCESS POLYGON API, ANOTHER TO CONVERT TIME STAMP FOR A DATA POINT.**\n",
    "From 20010-01 to 2020-12 A total 120 Months.\n",
    "The function access the Polygon API in 1 hour timeframes in a span of a monnth.\n",
    "Each Monthly Batch comes in as a JSON file which is converted to a pandas DataFrame then Converted into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_to_datetime(ts) -> str:\n",
    "    return datetime.datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "def print_csv(start, end, year, month):\n",
    "    # API key\n",
    "    key = '7FYtfcFojteNAujTh8pPfFoZnHnCl89E'  \n",
    "\n",
    "    endpoint = 'https://api.polygon.io/v2/aggs/ticker/C:GBPJPY/range/1/hour/{dfrom}/{to}?apiKey={key}&limit=50000'\n",
    "    resp = requests.get(endpoint.format(dfrom = start, to = end, key = key ))\n",
    "\n",
    "    my_dictionary = {'t': [' '], 'o': [0.0] , 'h': [0.0], 'l': [0.0], 'c': [0.0], 'v': [0.0], 'vw': [0.0], 'avgOHLC': [0.0]}\n",
    "\n",
    "    for result in resp.json()['results']:\n",
    "        dt = ts_to_datetime(result[\"t\"])\n",
    "        avgOHLC = (result['o'] + result['h'] + result['l'] + result['c']) / 4\n",
    "\n",
    "        my_dictionary['t'].append(dt)             # Time\n",
    "        my_dictionary['o'].append(result['o'])    # Open Price\n",
    "        my_dictionary['h'].append(result['h'])    # High Price\n",
    "        my_dictionary['l'].append(result['l'])    # Low Price\n",
    "        my_dictionary['c'].append(result['c'])    # Close Price\n",
    "        my_dictionary['v'].append(result['v'])    # Volume\n",
    "        my_dictionary['vw'].append(result['vw'])  # Volume Weighted\n",
    "        my_dictionary['avgOHLC'].append(avgOHLC)  # Average Price\n",
    "\n",
    "    # converts dictionary to pandas DataFrame\n",
    "    df = pd.DataFrame(my_dictionary)            \n",
    "  \n",
    "    filename = \"/drive/My Drive/CSV2/\" + year + \"-\" + month + \".csv\"\n",
    "\n",
    "    # converts pandas DataFrame into a csv file\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e305dc4",
   "metadata": {},
   "source": [
    "## **1.2 SCRIPT FOR AUTOMATING A TIMELINE OF 10 YEARS MONTHLY API CALLS**\n",
    "Resulting in 120 Monthly CSV Files with each file containing 1 hour data points in a span of a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training data timeline: 10 years\n",
    "# from 2010 to 2020\n",
    "\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "year = 2010\n",
    "month = 1\n",
    "\n",
    "for i in range(0, 11):\n",
    "\n",
    "    for j in range(0, 12):\n",
    "        _, num_days = calendar.monthrange(year + i, month + j)\n",
    "\n",
    "        start_day = datetime.date(year + i, month + j, 1)\n",
    "        end_day = datetime.date(year + i, month + j, num_days)\n",
    "\n",
    "        print_csv(start_day, end_day, str(year + i), str(month + j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6008cb",
   "metadata": {},
   "source": [
    "## **1.3 CONCATENATING ALL 120 MONTHLY CSV FILES INTO ONE CSV FILE.**\n",
    "The resulting csv file has 68,293 data points from 2010-01 to 2020-12 in 1 hour time frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f368769",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "os.chdir('/drive/My Drive/CSV2/')\n",
    "\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "# combines all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "# export to csv\n",
    "combined_csv.to_csv( \"forexDataGBPJYP.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581b046",
   "metadata": {},
   "source": [
    "## **1.4 UPLOADING CSV FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e25e60e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m \u001b[43mfiles\u001b[49m\u001b[38;5;241m.\u001b[39mupload()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e5143",
   "metadata": {},
   "source": [
    "## **1.5 DATA PREPROCESSING FOR MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e48a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp to datetime converter as polygon returns timestamps\n",
    "def ts_to_datetime(ts) -> str:\n",
    "    return datetime.datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "#Define the key for API and initialize all Vectors to be used for processing\n",
    "key_ = 'l9ZQM35caI2icwldDuRFLbntmAtW7zGKtdbw41'  \n",
    "dateVector = []\n",
    "ohlcVector = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f1ded",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **1.6 DATA SELECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14517e30",
   "metadata": {},
   "source": [
    "## **1.7 OPTIONAL CELL TO FETCH DATA FROM API INSTEAD OF DATABASE**\n",
    "Run this cell to populate the ohlcVector with average OHLC (open, high, low,  close) prices  obtained from the API. This works the best with daily data due to the way the Polygon API aggregates prices and allows for the flexibility of choosing any currency pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ = \"2009-10-01\"\n",
    "to_ = \"2020-05-01\"\n",
    "endpoint = 'https://api.polygon.io/v2/aggs/ticker/C:USDJPY/range/1/day/{dfrom}/{to}?unadjusted=true&sort=asc&apiKey={key}&limit=50000'\n",
    "resp = requests.get(endpoint.format(dfrom = from_, to = to_, key = key_ ))\n",
    "\n",
    "for result in resp.json()['results']:  \n",
    "    dt = ts_to_datetime(result[\"t\"])\n",
    "    avgOHLC = (result['o'] + result['h'] + result['l'] + result['c']) / 4\n",
    "    dateVector.append(dt[5:])\n",
    "    ohlcVector.append(float(avgOHLC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26502568",
   "metadata": {},
   "source": [
    "## **1.8 CELL TO FETCH DATA FROM UPLOADED FILE OR DATABASE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8686f9",
   "metadata": {},
   "source": [
    "### **1.8.1 CELL TO FETCH DATA FROM UPLOADED FILE OR DATABASE**\n",
    "If the cell above was not run, run this cell to use data from an uploaded csv file or database. This was necessary as we had to write a script to concatenate all 4 hour data and 1 hour data for a given currency from the API due to restrictions with how data is aggregated on the API side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create price difference vector to be used as input\n",
    "df2 = pd.read_csv(io.BytesIO(uploaded['FOREXdataGBP-JYP.csv'])) \n",
    "ohlcVector = df2['avgOHLC'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec7404",
   "metadata": {},
   "source": [
    "### **1.8.2 FETCH DATA FROM MONGODB MLAB DATABASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191bd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint \n",
    "\n",
    "mongo_client = MongoClient(\"mongodb://Moh:newpassword@cluster0-shard-00-00.iq4y5.mongodb.net:27017,cluster0-shard-00-01.iq4y5.mongodb.net:27017,cluster0-shard-00-02.iq4y5.mongodb.net:27017/DataMining2021?ssl=true&replicaSet=atlas-oqyrfh-shard-0&authSource=admin&retryWrites=true&w=majority\")\n",
    "\n",
    "# database name\n",
    "db = mongo_client[\"DataMining2021\"]\n",
    "# collection name\n",
    "col = db[\"4HRGBPYEN\"]\n",
    "\n",
    "result = col.find()\n",
    "\n",
    "openValue = []\n",
    "highValue = []\n",
    "lowValue = []\n",
    "closeValue = []\n",
    "avgOHLCValue = []\n",
    "for doc in col.find():\n",
    "    openValue += [doc[\"1\"]]\n",
    "    highValue += [doc[\"2\"]]\n",
    "    lowValue += [doc[\"3\"]]\n",
    "    closeValue += [doc[\"4\"]]\n",
    "    avgOHLCValue += [doc[\"7\"]]\n",
    "\n",
    "print(\"avgOHLCValue: \", closeValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c3675",
   "metadata": {},
   "source": [
    "# **PRICE PREDICTION OVER TIME MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d2d39",
   "metadata": {},
   "source": [
    "## **2.1 GENERATING CHANGES IN PRICE AND SPLITTING DATA FOR TRAINING AND TESTING**\n",
    "Instead of using prices as inputs to our model, which are sensitive to trends due to time, we use percent changes in price which do not change as prices go up or down. This also helps to provide bounds to the data. For our splitting of the data, we use a standard 80/20 split between training and testing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70469e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpriceVector = []\n",
    "\n",
    "for i in range(0,len(ohlcVector)-1):\n",
    "    dpriceVector.append(float('{:.7f}'.format(((ohlcVector[i+1]-ohlcVector[i])/ohlcVector[i])))*100)\n",
    "\n",
    "vocab_size = len((set(dpriceVector)))\n",
    "\n",
    "print(\"TOTAL SIZE:\", len(dpriceVector))\n",
    "print(\"There are\", vocab_size, \"unique characters in the dataset\")\n",
    "\n",
    "split_point = int(len(dpriceVector)*0.8)\n",
    "trainingData = dpriceVector[:split_point]    \n",
    "testData = dpriceVector[split_point:]\n",
    "trainingData_len = len(trainingData)\n",
    "testData_len = len(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034d75e",
   "metadata": {},
   "source": [
    "## ** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861bc17",
   "metadata": {},
   "source": [
    "## **2.3 SHAPING DATA FOR MODEL**\n",
    "The idea for our model is that it takes a sequence of percent changes in price and tries to predict the next percent change. Therefore, the input for our model needs to be a list of percent change sequences and our output will be a list of what the next percent change should be. Since we need to define the sequence length, we will define this function now and use it later when we define hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingBatch(seq_length):\n",
    "  x_train = []\n",
    "  y_train = []\n",
    "  for x in range(seq_length, trainingData_len):\n",
    "    x_train.append(trainingData[x-seq_length:x])\n",
    "    y_train.append(trainingData[x])\n",
    "  x_train = np.array(x_train)\n",
    "  y_train = np.array(y_train)\n",
    "  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "  return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a37e9",
   "metadata": {},
   "source": [
    "## **2.4 BUILDING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10dcf8d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2142836946.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    <html>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<html>\n",
    "<head>\n",
    "<h1>Recurrrent Neural Network using Long Short Term Memory (LSTM) </b></h1>\n",
    "<br>\n",
    "<img src=\"https://i.ibb.co/y5FDfpW/LSTM.png\" alt=\"LSTM\" border=\"0\">\n",
    "<img src=\"https://i.ibb.co/v1cFZch/LSTM5.png\" alt=\"LSTM5\" border=\"0\">\n",
    "<img src=\"https://i.ibb.co/6bMHPN2/Many-To-One.png\" alt=\"Many-To-One\" border=\"0\">\n",
    "</head>\n",
    "\n",
    "<html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d63924",
   "metadata": {},
   "source": [
    "### **2.4.1 MODEL ARCHITECTURE**\n",
    "This function defines the architecture our model uses. We stack 3 LSTMs to provide layers of abstraction that allow the model to create its own intermediary data from which it will generate a prediction through the final dense layer with 1 output. We placed dropout layers between each LSTM layer to help prevent overfitting. The Model uses 1 dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f87ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(rnn_units, input_shape_):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, input_shape=input_shape_),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(rnn_units),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e72f9",
   "metadata": {},
   "source": [
    "### **2.4.2 SUMMARY OF A SAMPLE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(256, (250, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3b9eb",
   "metadata": {},
   "source": [
    "### **2.4.3 DEFINING HYPERPARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0264d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters:\n",
    "num_epochs = 120\n",
    "batch_size_ = 32  \n",
    "seq_length = 125 \n",
    "learning_rate = 1e-5 \n",
    "\n",
    "# Model parameters: \n",
    "rnn_units = 150  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd57cf",
   "metadata": {},
   "source": [
    "### **2.4.4 TRAINING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d909fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the training batch \n",
    "x_train,y_train = createTrainingBatch(seq_length)\n",
    "#build the model\n",
    "model = build_model(rnn_units, (x_train.shape[1],1))\n",
    "#define optimizer and loss function\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "#fit the model to the data\n",
    "history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478a51f",
   "metadata": {},
   "source": [
    "### **2.4.8 PLOTTING THE LOSS THROUGHOUT TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8db6c",
   "metadata": {},
   "source": [
    "# **3 TESTING THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726fae0",
   "metadata": {},
   "source": [
    "## **3.1 TEST DATA GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_length = 10\n",
    "x_test = []\n",
    "for x in range(seq_length,len(dpriceVector[0:-(generation_length)])):\n",
    "  x_test.append(dpriceVector[x-seq_length:x])\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ae09a",
   "metadata": {},
   "source": [
    "## **3.2 PRICE PREDICTION**\n",
    "One problem we saw with many model online is how they handle prediction. Since the goal is predicting prices into the future, the model should be making prediction based off its own predictions. Many online models simply run the model on the test data which does not accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price = []\n",
    "for i in range(0,generation_length):\n",
    "  predicted_price = model.predict(x_test)\n",
    "  print(predicted_price[-1][0])\n",
    "  add_price = float('{:.6f}'.format(predicted_price[-1][0]))\n",
    "  x_test_copy = x_test.copy()\n",
    "  np.squeeze(x_test_copy)\n",
    "  x_test_copy = np.append(x_test_copy,add_price)\n",
    "  new_data_sequence = x_test_copy[(0-seq_length):]\n",
    "  new_data_sequence = new_data_sequence.reshape(1,seq_length,1)\n",
    "  x_test = np.concatenate((x_test,new_data_sequence))\n",
    "predicted_prices = [predicted_price[i] for i in range(0,len(predicted_price))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff90fa",
   "metadata": {},
   "source": [
    "## **3.3 CONVERTING PERCENT PRICE CHANGES BACK TO ACTUAL PRICES**\n",
    "Since our model outputs percent changes in price, we must convert this back to actual prices given a starting price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe38e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculatedOriginalPrices = []\n",
    "calculatedOriginalPrices.append(ohlcVector[seq_length])\n",
    "\n",
    "for i in range(seq_length,len(dpriceVector)):\n",
    "    nextPriceOriginal = calculatedOriginalPrices[i-seq_length] * ((dpriceVector[i])/100+1)\n",
    "    calculatedOriginalPrices.append(nextPriceOriginal)\n",
    "\n",
    "calculatedPredictedPrices = []\n",
    "calculatedPredictedPrices.append(ohlcVector[seq_length])\n",
    "\n",
    "for i in range(0,len(predicted_prices)):\n",
    "    nextPricePredicted = calculatedPredictedPrices[i] * ((predicted_prices[i])/100+1)\n",
    "    calculatedPredictedPrices.append(nextPricePredicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb76680",
   "metadata": {},
   "source": [
    "## **3.4 VISUALIZING THE RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775251e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dpriceVector[seq_length:], color='black', label=\"Actual price movement\")\n",
    "plt.plot(predicted_prices, color='red', label=\"Predicted price movement\")\n",
    "plt.title(\"Price change\")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel(\"Price Change\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(calculatedOriginalPrices, color='black', label=\"Actual prices\")\n",
    "plt.plot(calculatedPredictedPrices, color='red', label=\"Predicted prices\")\n",
    "plt.title(\"Price\")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbbcee",
   "metadata": {},
   "source": [
    "# **4 SHORT TERM CATEGORICAL PRICE PREDICTION MODEL**\n",
    "After experimenting with the price prediction over time model, we came to the conclusion that predicting prices over the long term is not feasible given the amount of external variables at play that compound over time. Instead, we decided to pursue a model that could be used as more of a trading tool. Based on previous prices, we try to predict whether the price will be moving up or down in the next time frame and how big will the movement be. Because the output is categorical, the model will also be able to provide confidence metric which we could leverage to find optimal times to enter the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701a00f",
   "metadata": {},
   "source": [
    "## **4.1 VISUALIZING TRAINING DATA THROUGH A HISTOGRAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trainingData, edgecolor=\"black\",bins=100)\n",
    "plt.xlim(-2,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21959769",
   "metadata": {},
   "source": [
    "## **4.2 DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34705fa9",
   "metadata": {},
   "source": [
    "## **4.3 VECTORIZING OUTPUTS AND SHAPING DATA FOR MODEL**\n",
    "Since we will be predicting categories of price movement, we must define what is considered a big, medium, and small movement. We first accomplished this by simply visualizing the data through a histogram and picking the appropriate cutoffs. These are the values we settled on for 4-hour data.\n",
    "\n",
    "0.  -> Big Move Down (:-2]\n",
    "1.  -> Medium Move Down (-2:-0.75]\n",
    "2.  ->Small Move Down (-0.75:0)\n",
    "3.  ->Small Move Up (0:0.75) \n",
    "4.  ->Medium Move Up [0.75:2)\n",
    "5.  ->Big Move Up [0.2:)\n",
    "6.  ->No movement 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCategoricalTrainingBatch(seq_length):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    high_cutoff = 0.5;\n",
    "    medium_cutoff = 0.25\n",
    "    \n",
    "    for x in range(seq_length, trainingData_len):\n",
    "        x_train.append(trainingData[x-seq_length:x])\n",
    "        if trainingData[x] <= -(high_cutoff):\n",
    "            y_train.append(0)\n",
    "        elif trainingData[x] <= -(medium_cutoff) and trainingData[x] > -high_cutoff:\n",
    "            y_train.append(1)\n",
    "        elif trainingData[x] < 0 and trainingData[x] > -(medium_cutoff):\n",
    "            y_train.append(2)\n",
    "        elif trainingData[x] > 0 and trainingData[x] < medium_cutoff:\n",
    "            y_train.append(3)\n",
    "        elif trainingData[x] >= medium_cutoff and trainingData[x] < high_cutoff:\n",
    "            y_train.append(4)\n",
    "        elif trainingData[x] >= high_cutoff:\n",
    "            y_train.append(5)\n",
    "        else:\n",
    "        y_train.append(6)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    return x_train, y_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f74699",
   "metadata": {},
   "source": [
    "# **5 BUILDING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58311800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2bbe15c",
   "metadata": {},
   "source": [
    "## **5.1 MODEL ARCHITECTURE**\n",
    "Our model for the cateogrical predictions is very similar to our initial model for price predictions over time. The big exception is that the output dense layer now has 7 outputs for each of the categories we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d528193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGED MODEL NEEDS TO HAVE A DENSE LAYER EQUAL TO NUMBER OF CATEGORIES WE MADE\n",
    "def build_model(rnn_units, seq_length):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(rnn_units, return_sequences=True, input_shape=(seq_length,1)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(rnn_units),\n",
    "    tf.keras.layers.Dense(7)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "###Categories:\n",
    "# Big Move Down\n",
    "# Medium Move Down\n",
    "# Small Move Down\n",
    "# No Movement\n",
    "# Small Move Up\n",
    "# Medium Move Up\n",
    "# Big Move Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456917a7",
   "metadata": {},
   "source": [
    "## **5.2 DEFINING HYPERPARAMTERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters:\n",
    "num_epochs_categorical = 100  \n",
    "batch_size_categorical = 32  \n",
    "seq_length_categorical = 125  \n",
    "learning_rate_categorical = 1e-5 \n",
    "\n",
    "# Model parameters: \n",
    "rnn_units_categorical = 150 \n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir_categorical = './training_checkpoints_categorical'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir_categorical, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09ccc6",
   "metadata": {},
   "source": [
    "## **5.3 GENERATING THE CATEGORICAL TRAINING BATCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = createCategoricalTrainingBatch(seq_length_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cc1c3",
   "metadata": {},
   "source": [
    "## **5.4 VISUALIZING CATEGORY DISTRIBUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, edgecolor=\"black\",bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba311e0",
   "metadata": {},
   "source": [
    "## **5.5 TRAINING THE MODEL**\n",
    "We train the model in much the same way as we trained the price prediction over time model except that we are using the Adamax optimizer which is more suited for categorical data and we are using Sparse Categorical Cross Entropy as the loss function (ensuring it applies a softmax over outputs before calculations are done with from_logits=True). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e46e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(rnn_units_categorical, seq_length_categorical)\n",
    "model.compile(optimizer='Adamax', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "history = model.fit(x_train, y_train, epochs=num_epochs_categorical, batch_size=batch_size_categorical)\n",
    "#model.save_weights(checkpoint_prefix)\n",
    "#files.download('/content/training_checkpoints_categorical/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684c113",
   "metadata": {},
   "source": [
    "## **5.6 PLOTTING THE LOSS THROUGHOUT TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2a2a0",
   "metadata": {},
   "source": [
    "## **5.7 Optional Building Model from Saved Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(rnn_units_categorical, seq_length_categorical)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b3d1a",
   "metadata": {},
   "source": [
    "# **6 TESTING THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28cf20",
   "metadata": {},
   "source": [
    "## **6.1 SIMPLE SINGLE DATA CHECKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "comparison_point = -5356\n",
    "for x in range(seq_length_categorical,len(dpriceVector[0:comparison_point])):\n",
    "  x_test.append(dpriceVector[x-seq_length_categorical:x])\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "predicted_price_movements = model.predict(x_test)\n",
    "softmax_results = tf.nn.softmax(predicted_price_movements)\n",
    "print(softmax_results[-1])\n",
    "predicted_move = tf.random.categorical(predicted_price_movements, num_samples=1)[-1,0].numpy()\n",
    "print(predicted_move)\n",
    "print(dpriceVector[comparison_point - 1])\n",
    "# FOR REFERENCE\n",
    "#0 - Big Move Down (:-2]\n",
    "#1 - Medium Move Down (-2:-0.75]\n",
    "#2 - Small Move Down (-0.75:0)\n",
    "#3 - Small Move Up (0:0.75) \n",
    "#4 - Medium Move Up [0.75:2)\n",
    "#5 - Big Move Up [2:)\n",
    "#6 - No Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab7f76",
   "metadata": {},
   "source": [
    "## **6.2 ACCURACY SCRIPT - LOOPING THROUGH TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61edb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for x in range(seq_length_categorical,len(dpriceVector)):\n",
    "  x_test.append(dpriceVector[x-seq_length_categorical:x])\n",
    "for x in range(0, testData_len):\n",
    "  if testData[x] <= -0.5:\n",
    "    y_true.append(0)\n",
    "  elif testData[x] <= -0.25 and testData[x] > -0.5:\n",
    "    y_true.append(1)\n",
    "  elif testData[x] < 0 and testData[x] > -0.25:\n",
    "    y_true.append(2)\n",
    "  elif testData[x] > 0 and testData[x] < 0.25:\n",
    "    y_true.append(3)\n",
    "  elif testData[x] >= 0.25 and testData[x] < 0.5:\n",
    "    y_true.append(4)\n",
    "  elif testData[x] >= 0.5:\n",
    "    y_true.append(5)\n",
    "  else:\n",
    "    y_true.append(6)\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "predicted_price_movements = model.predict(x_test)\n",
    "raw_test_predictions = predicted_price_movements[(split_point-seq_length_categorical):]\n",
    "y_pred = [tf.random.categorical([predicted_category], num_samples=1).numpy() for predicted_category in raw_test_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee2cca",
   "metadata": {},
   "source": [
    "## **6.3 MODEL ACCURACY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL ACCURACY\n",
    "num_correct = 0;\n",
    "for i in range(0, len(y_pred)):\n",
    "  if y_pred[i] == y_true[i]:\n",
    "    num_correct +=1 \n",
    "\n",
    "print(\"TOTAL CORRECT:\",num_correct)\n",
    "print(\"TOTAL DATA:\", len(y_pred))\n",
    "print(\"Accuracy:\", num_correct/len(y_pred))\n",
    "\n",
    "#RANDOM CHOICE ACCURACY\n",
    "num_correct = 0;\n",
    "for i in range(0,len(y_pred)):\n",
    "  if y_true[i] == random.randint(0,6):\n",
    "    num_correct += 1\n",
    "print(\"TOTAL CORRECT:\",num_correct)\n",
    "print(\"TOTAL DATA:\", len(y_pred))\n",
    "print(\"Accuracy:\", num_correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9dd37",
   "metadata": {},
   "source": [
    "## **6.4 CONFUSION MATRIX AND OTHER METRICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_true, np.squeeze(y_pred))\n",
    "sns.heatmap(cf_matrix, annot=True)\n",
    "print(classification_report(y_true, np.squeeze(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da34a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbfdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
